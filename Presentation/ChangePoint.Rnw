\subsection{Goal and Model}
\subsection*{Goal}
  \frame{\frametitle{Change point detection context}
    \begin{tabular}{p{0.5\textwidth} p{0.4\textwidth}}
    \begin{tabular}{p{\textwidth}}
        \includegraphics<1>[height=.6\textheight]{rawSignal}
        \includegraphics<2>[height=.6\textheight]{segmentedSignal}
      \end{tabular}
    &  
      \begin{tabular}{p{0.4\textwidth}}
        \onslide<1>{ \emph{Goal : } Identifying homogenous region and abrupt changes in the signal.\\}
        \onslide<2>{ These \textcolor{red}{regions} may be interpreted afterwards.\\}
    \end{tabular}
  \end{tabular}
  }

  \frame{\frametitle{Change point detection context}

  Two possible statistical point of view : 
  \begin{itemize}
  \item Bayesian approach : the output is the posterior probability for each point to be a change point.
  \item \emph{Frequentist} approach : the output is the best segmentation (according to a given critrion)
  \end{itemize}
  }

\subsection*{Model}
\frame{\frametitle{Underlying model}
  \only<1>{ General framework~:}
  \only<2>{\textcolor{red}{Change point detection in the trend~:} }
  \begin{itemize}
  \item Data $Y_1,\ldots,Y_n$ are drawn from a given pdf, driven by unknown parameter $\theta$
    \begin{equation*}
      Y_t \sim f_{\theta}(.)  \ \ 
    \end{equation*}
  \item
    $\theta$ values change at  $K-1$ unknow instants, the change point~:
    $t_1,\ldots,t_{K-1}$ :
    \only<1>{
      \begin{equation*}
        Y_t \sim f(\theta_k) \ \ \mbox{if $t$ in region $I_k=[t_{k-1}+1,t_{k}]$}
      \end{equation*}}
    \only<2>{
      \textcolor{red}{
        \begin{equation*}
          Y_t=\mu_k + E_t \quad \{E_t\} \mbox{i.i.d.} \sim \Ncal(0,\sigma^2) \mbox{ if $t$ in portion $I_k$,}
        \end{equation*}
        for  $k=1,\ldots,K$.
      } }
 \par
 \medskip
\noindent Remark : $K-1$ change points  $\Leftrightarrow$ $K$ regions.
\end{itemize}
 } 

 
 \subsection*{Estimation}
 \frame{\frametitle{Estimation procedure}
   \begin{itemize}
   \item Unknown parameters : $\mu_1, \ldots, \mu_K$, $\sigma$, and $T_1, \ldots, T_K$,\\
     but  also \textcolor{red}{K} itself.
\pause
   \item Estimation Procedure
     \begin{itemize}
       \item for a given $K$,  $\mu_1, \ldots, \mu_K$, $\sigma_1, \ldots,\sigma_K$, and $T_1, \ldots, T_K$ are estimated using maximum likelihood (and dynanic programming)
       \item K is chosen using penalized likelihood approach
       \end{itemize}
     \end{itemize} 
         \pause
         \begin{tabular}{p{0.4\textwidth} p{0.4\textwidth}}
           \begin{tabular}{p{0.4\textwidth}}
             Likelihood increases with the number of segment, but
           \end{tabular}
           &  \pause
           \begin{tabular}{p{0.4\textwidth}}
             \includegraphics[scale=0.3]{LikelihoodProfile}
           \end{tabular}
         \end{tabular}
   }


\frame{\frametitle{Estimation procedure}

\paragraph{Likelihood}
{\small
\begin{eqnarray*}
2 log(P_K (T, \theta)) & = & 2 \sum_{k=1}^K \log f(\{Y_t\}_{t \in
I_k};
\theta_k) = 2 \sum_{k=1}^K \sum_{t \in I_k}\log f(Y_t; \theta_k) \\
& = & -n \log \sigma^2 - \frac1{\sigma^2} \sum_{k=1}^K \sum_{t \in
  I_k} (Y_t - \mu_k)^2 + \mbox{cst}.
\end{eqnarray*}
}

\paragraph{Estimations}
\begin{equation*}
 (\hat{T},\hat{\theta})=
  \argmax_{(T,\theta)} log(P_K (T, \theta))
\end{equation*}

\paragraph{If the change points are known}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{equation*}
\widehat{\mu}_k = \frac1{n_k} \sum_{t \in I_k} Y_t
\end{equation*}
\end{column}
\begin{column}{0.5\textwidth}
  $$
  \widehat{\sigma}^2 =  \frac1{n} \sum_{k=1}^K \sum_{t \in I_k} (Y_t -  \widehat{\mu}_k)^2
  $$
\end{column}
\end{columns}
}

\begin{frame}[fragile]{Finding the K-1 change points}
\only<1-3>{
\paragraph{Considering all possible segmentations, } the best segmentation minimizes 
$$
J_k(1, n) = \sum_{k=1}^K \sum_{t \in I_k} (Y_t - \widehat{\mu}_k)^2.
$$
\pause
\paragraph{But, } $$\left( \begin{array}{c} n-1 \\ K-1 \end{array}\right)$$ possible choices for the $K-1$ positions
:\\
  \centerline{$\Rightarrow$ Practically impossible even for small $K$ and $n$}
  \medskip 
  \pause
  
  $K=10$, $n=200$,  $\left( \begin{array}{c} K-1 \\ n-1 \end{array}\right)\approx 2. 10^{16}$
}

\paragraph{Dynamic programming,} with complexity ($\mathcal{O}(n^2)$).


%possible only because the quantity of interest is the sum over all segments 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\only<4>{\centerline{\sl Sub-paths of the optimal path are themselves
optimal,} 
Bellmann optimality 

\vspace{3.8cm}
$$ $$ }
\only<5>{
\begin{description}

\item[Initialisation:] Compute for $0 \leq i < j \leq n$, cost of portion $I_{ij}$~:
 $$
 J_1(i, j) = \sum_{t=i+1}^j (Y_t - \widehat{\mu})^2
 $$
\item[Etape $k$:] Compute for $2 \leq k \leq K$, $J_k(i, j)$ the cost of the best segmentation in $k$ segments between $i$ and $j$.
  $$
  J_k(i, j) = \min_{i < h <j} \left[J_{k-1}(i, h) + J_1(h+1,  j)\right].
  $$
\end{description}
}
\end{frame}


\subsection{Example}
\subsection*{ Toy Example}
   \begin{frame}[fragile]{How to perform this segmentation approach ?}
<<echo=FALSE, cache=FALSE>>=
read_chunk("../RCode/LectureSegmentation-RIllustration.R")
@
 
\begin{columns}
\begin{column}{0.4\textwidth}
<<segCode2, fig.show='hide'>>=
@
\end{column}
\begin{column}{0.5\textwidth}
\includegraphics[scale=0.35]{segCode2-1.pdf}
\end{column}
\end{columns}
\end{frame}

\subsection*{Practical Example}
   \begin{frame}[fragile]{How to perform this segmentation approach ?}
\begin{columns}
\begin{column}{0.4\textwidth}
<<segCode3, fig.show='hide', results='hide'>>=
@
\end{column}
\begin{column}{0.5\textwidth}
\includegraphics[scale=0.35]{segCode3-1.pdf}
\end{column}
\end{columns}
\end{frame}
% 

 
 \begin{frame}[fragile]{Do it yourself}
 <<echo=FALSE, cache=FALSE>>=
read_chunk("../RCode/SegmentationExample.R")
 @ 
\begin{columns}
\begin{column}{0.4\textwidth}
<<Practical1, fig.show='hide', results='hide'>>=
option(width=50)
@
\end{column}
\begin{column}{0.5\textwidth}
\includegraphics[scale=0.35]{Practical1-1.pdf}
\end{column}
\end{columns}
\end{frame}

\subsection{ClusteringSegmentation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{When Segmentation is not sufficient}
$$
\begin{tabular}{cc}
  Pure segmentation & Segmentation + classification \\
  \includegraphics[scale=0.3]{FigSegClas-1.pdf} &
  \includegraphics[scale=0.3]{FigSegClas-2.pdf} 
  \end{tabular}
$$
\end{frame}
\begin{frame}{Segmentation-Clustering}
 \begin{itemize}
 \item Assuming a \textcolor{blue}{secondary underlying
     structure} of the segments into $P$ groups with weights
   $\pi_1,...,\pi_P (\sum_p \pi_p=1)$.
 \item Let's define \emphase{hidden variables $Z_{kp}$}, indicators of the
   \emphase{group to which segment $k$ belongs}.
 \item $\pi_p$ denotes the \emphase{proportion} of group $p$.
 \item The \emphase{distribution of the signal} given the group of the
     segment is
  \begin{align*}
   t \in I_k, k \in p &\qquad \Rightarrow \qquad Y_t \sim
   \Ncal(m_{\emphase{p}}, \sigma^2)\\
   Y^k|Z_{kp}=1 &\sim \Ncal( m_p, \sigma^2 ).
   \end{align*}
 %\item It is a model of \textblue{segmentation/clustering}.
 \item The \emphase{parameters of this model} are
   \begin{eqnarray*}
   \mbox{the breakpoint positions:} \quad T&=&(t_1, ..., t_{K-1}),\\
     \mbox{the mixture characteristics:} \quad \Theta&=&(\pi_1,\hdots,\pi_P;\mu_1,\hdots,\mu_P,\sigma).
   \end{eqnarray*}
\end{itemize}
\end{frame} 


\begin{frame}{Hybrid algorithm}
\only<1>{
\paragraph{2 levels of statistical units} 
 \begin{itemize}
 \item  The inference of the \emphase{breakpoints $T$}
   is made at the \emphase{position level $t$};
 \item  The inference of the \emphase{groups (status)
     ($\Theta, \tau_{kp}$)} is made at the \emphase{segment level $k$}.
 \end{itemize}
} 
\only<2>{
 \paragraph{Alternate parameters estimation with $K$ and $P$ known}
 \begin{enumerate}
 \item  When $T$ is fixed, the
   \textcolor{blue}{Expectation-Maximisation (EM)} algorithm estimates
   $\Theta$;
   $$
   \hat{\Theta}^{(h+1)}=\underset{\Theta}{\arg\max} \left\{\log
     \Lcal_{KP}\left(\Theta,T^{(h)}\right) \right\}. 
  $$
   $$
   \log \Lcal_{KP}( \hat{\Theta}^{(h+1)}; \hat{T}^{(h)})
   \geq \log \Lcal_{KP}(\hat{\Theta}^{(h)};
  \hat{T}^{(h)})
  $$
 \item  When $\Theta$ is fixed, \textcolor{blue}{dynamic
     programming} estimates $T$;
    $$
   \hat{T}^{(h+1)}=\underset{T}{\arg\max} \left\{\log
     \Lcal_{KP}\left(\hat{\Theta}^{(h+1)},T\right) \right\}. 
  $$
   $$
   \log \Lcal_{KP}(\hat{\Theta}^{(h+1)}; \hat{T}^{(h+1)})
   \geq \log \Lcal_{KP}(\hat{\Theta}^{(h+1)};
   \hat{T}^{(h)})
   $$
   \end{enumerate} 
}
\end{frame}
 

\subsection{Example}
\subsection*{Toy Example}
   \begin{frame}[fragile]{How to perform this segmentation/clustering approach ?}
 
\begin{columns}
\begin{column}{0.4\textwidth}
<<segCode4, fig.show='hide', results='hide'>>=
@
\end{column}
\begin{column}{0.5\textwidth}
\includegraphics[scale=0.35]{segCode4-1.pdf}
\end{column}
\end{columns}
\end{frame}

\subsection*{Practical Example}
\begin{frame}[fragile]{Do it yourself}
\end{frame}
